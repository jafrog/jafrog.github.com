<!DOCTYPE HTML>
<head>
  <meta http-equiv='Content-Type' content='text/html; charset=utf-8' />
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=IBM+Plex+Mono:wght@400;500;600&display=swap" rel="stylesheet">

  <link rel="stylesheet" href="/css/main.css" type="text/css" media="screen" />
  <link rel="stylesheet" href="/css/syntax.css" type="text/css" media="screen" />
  <link rel="shortcut icon" type="image/png" href="/assets/favicon.png">
  <!--  <link rel="stylesheet" href="/css/pygments.css" type="text/css" media="screen" />-->

  <!-- Google analytics -->
  <script type="text/javascript">

    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-30366699-2']);
    _gaq.push(['_trackPageview']);

    (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();

  </script>

  <title>jafrog</title>
</head>

<body>
  <div class="mx-auto max-w-7xl sm:px-6 lg:px-8 bg-slate-50 font-plex">
    <div class="mx-auto max-w-5xl sm:px-8 lg:px-24">
      <div class="flex justify-between sm:py-6 md:py-12 border-b">
    <div class="flex flex-col justify-start">
        <h1 class="text-3xl font-semibold">
            <a href="/">jafrog's dev blog</a>
        </h1>
        <div class="flex justify-start items-center mt-2`">
            <a href="https://www.github.com/jafrog">
                <img src="/assets/github.svg" alt="Github" class="w-6 h-6" />
            </a>
            <a href="https://www.linkedin.com/in/irinabednova/">
                <img src="/assets/linkedin.svg" alt="Linkedin" class="w-6 h-6">
            </a>
        </div>
    </div>
    <div class="right-0 flex items-center">
        <img src="/assets/avatar.jpg" alt="jafrog" class="h-16 w-16 rounded-full" />
    </div>
</div>

      <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<script src="/assets/snap.svg.js"></script>
<div class="pt-20">
  <div class="flex items-center pb-2">
    <img src="/assets/calendar.svg" alt="Time" class="w-4 h-4" />
    <time class="pl-1 text-gray-500" datetime="2024-04-29 00:00:00 +0000">29 April 2024</time>
  </div>
  <div class="text-3xl pb-10 font-medium text-sky-500">Building A Neural Network From Scratch</div>
</div>
<div class="pb-20">
  <h1 id="prerequisites">Prerequisites</h1>

<p>This walkthrough assumes that you understand the core concepts of Neural Networks. Here‚Äôs the non-exhaustive list:</p>

<ul class="list-disc pl-4 pb-4 leading-loose">
<li>Logistic regression</li>
<li>Gradient descent</li>
<li>Forward and backward propagation</li>
<li>Loss functions</li>
</ul>

<p>If you understand all of the above ‚Äúin principle‚Äù, but struggle to turn the concepts into code, this post is for you.</p>

<p>This is based on the material covered in the first course of the <a href="https://www.coursera.org/specializations/deep-learning">Deep Learning Specialisation</a> on Coursera. I highly recommend enrolling in this course if you want to learn the fundamentals of Neural Networks. The first course in particular is a hoot.</p>

<h1 id="the-goal">The goal</h1>

<p>By the end of this post we should have a working Neural Network that can classify the images of beans as ‚Äúheathy‚Äù or ‚Äúnot healthy‚Äù. The point is to understand the building blocks of a Neural Networks without using the frameworks like Tensorflow or PyTorch.</p>

<p>I particularly struggled with getting a good intuition for back propagation.</p>

<h1 id="next-steps">Next steps</h1>

<p>In the next post we‚Äôll change the classifier to a multi-class one using the softmax function.</p>

<h1 id="the-implementation">The implementation</h1>

<h2 id="download-the-dataset">Download the dataset</h2>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
</pre></td><td class="rouge-code"><pre><span class="c1"># datasets is a ü§ólibrary to load and manipulate datasets
</span><span class="kn">from</span> <span class="nn">datasets</span> <span class="kn">import</span> <span class="n">load_dataset</span>

<span class="c1"># Loads only the training examples from the dataset
</span><span class="n">dataset</span> <span class="o">=</span> <span class="n">load_dataset</span><span class="p">(</span><span class="s">"beans"</span><span class="p">,</span> <span class="n">split</span><span class="o">=</span><span class="s">"train"</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="s">"Dataset shape: "</span><span class="p">,</span> <span class="n">dataset</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
<span class="n">dataset</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>Output:</p>

<div class="bg-gray-100 py-4 mb-6">
    <span>
    Dataset shape:  (1034, 3)
    <br />
    <br />
    &lcub;
    <br />
        &#32;&#32;'image_file_path': '.../train/angular_leaf_spot/angular_leaf_spot_train.0.jpg',
    <br />
        &#32;&#32;'image': &lt;PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=500x500&gt;,
    <br />
        &#32;&#32;'labels': 0
    <br />
    &rcub;
    </span>
</div>

<p>The shape of the dataset is <code class="language-plaintext highlighter-rouge">(1034,3)</code> - 1034 examples, each with 3 features: <code class="language-plaintext highlighter-rouge">image_file_path</code>, <code class="language-plaintext highlighter-rouge">image</code> and <code class="language-plaintext highlighter-rouge">labels</code>.
The first example in the dataset has a label 0. To check what this label corresponds to, we can check the features attribute of the dataset.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre></td><td class="rouge-code"><pre><span class="n">dataset</span><span class="p">.</span><span class="n">features</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>Output:</p>

<div class="bg-gray-100 py-4 mb-6">
    &lcub;
    <br />
    'image_file_path': Value(dtype='string', id=None),
    <br />
    'image': Image(decode=True, id=None),
    <br />
    'labels': ClassLabel(names=['angular_leaf_spot', 'bean_rust', 'healthy'], id=None)
    <br />
    &rcub;
</div>

<p>This dataset has 3 classes: ‚Äòangular_leaf_spot‚Äô = 0; ‚Äòbean_rust‚Äô = 1; ‚Äòhealthy‚Äô = 2. The first example is an image of a bean leaf with angular leaf spot.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre></td><td class="rouge-code"><pre><span class="n">dataset</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s">"image"</span><span class="p">]</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>Output:</p>

<p><img src="/assets/posts/simple-nn-binary/bean_leaf.png" alt="Angular leaf spot" /></p>

<p>To convert the image into an input for the NN, we need to convert it into a feature vector. An RGB image is a 3D tensor with shape <code class="language-plaintext highlighter-rouge">(height, width, channels)</code>. We can flatten this tensor into a 1D vector of shape <code class="language-plaintext highlighter-rouge">(height * width * channels,)</code>.</p>

<p>Convert one image to a feature vector. Each image is 500px x 500px and RGB-encoded.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
</pre></td><td class="rouge-code"><pre><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">ImageFeatureExtractionMixin</span>

<span class="n">feature_extractor</span> <span class="o">=</span> <span class="n">ImageFeatureExtractionMixin</span><span class="p">()</span>
<span class="n">feature_vector</span> <span class="o">=</span> <span class="n">feature_extractor</span><span class="p">.</span><span class="n">to_numpy_array</span><span class="p">(</span><span class="n">dataset</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s">"image"</span><span class="p">],</span> <span class="n">channel_first</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Original image shape: "</span><span class="p">,</span> <span class="n">feature_vector</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Feature vector shape: "</span><span class="p">,</span> <span class="n">feature_vector</span><span class="p">.</span><span class="n">flatten</span><span class="p">().</span><span class="n">shape</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>Output:</p>

<div class="bg-gray-100 py-4 mb-6">
    <span>
    Original image shape:  (500, 500, 3)
    <br />
    Feature vector shape:  (750000,)
    </span>
</div>

<p>Iterate over the dataset and convert all images to feature vectors.</p>

<p>We will also resize images so the feature vectors are not as long (<code class="language-plaintext highlighter-rouge">(30000,)</code> as opposed to <code class="language-plaintext highlighter-rouge">(750000,)</code>)</p>

<p><code class="language-plaintext highlighter-rouge">to_numpy_array</code> also normalizes the RGB values by default, making them between 0 and 1.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
</pre></td><td class="rouge-code"><pre><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="k">def</span> <span class="nf">transform</span><span class="p">(</span><span class="n">examples</span><span class="p">):</span>
    <span class="n">examples</span><span class="p">[</span><span class="s">"feature_vector"</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="n">feature_extractor</span><span class="p">.</span><span class="n">to_numpy_array</span><span class="p">(</span><span class="n">image</span><span class="p">.</span><span class="n">convert</span><span class="p">(</span><span class="s">"RGB"</span><span class="p">).</span><span class="n">resize</span><span class="p">((</span><span class="mi">100</span><span class="p">,</span><span class="mi">100</span><span class="p">)),</span> <span class="n">channel_first</span><span class="o">=</span><span class="bp">False</span><span class="p">).</span><span class="n">flatten</span><span class="p">()</span> <span class="k">for</span> <span class="n">image</span> <span class="ow">in</span> <span class="n">examples</span><span class="p">[</span><span class="s">"image"</span><span class="p">]]</span>
    <span class="k">return</span> <span class="n">examples</span>

<span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">.</span><span class="nb">map</span><span class="p">(</span><span class="n">transform</span><span class="p">,</span> <span class="n">batched</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">dataset</span><span class="p">.</span><span class="n">shape</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>Output:</p>

<div class="bg-gray-100 py-4 mb-6">
    <span>
    (1034, 4)
    </span>
</div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
</pre></td><td class="rouge-code"><pre><span class="c1"># Convert dataset into the input for the NN
</span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="n">dataset</span><span class="p">[</span><span class="s">"feature_vector"</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="s">'float32'</span><span class="p">).</span><span class="n">T</span>
<span class="k">print</span><span class="p">(</span><span class="s">"X: "</span><span class="p">,</span> <span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>Output:</p>

<div class="bg-gray-100 py-4 mb-6">
    <span>
    X:  (30000, 1034)
    </span>
</div>

<h2 id="binary-classifier">Binary classifier</h2>

<p>Let‚Äôs change the labels to ‚Äúsick‚Äù and ‚Äúnot sick‚Äù to turn this into a binary classifier.</p>

<p>If the label is ‚Äú2‚Äù the value is 0 (not sick). If it‚Äôs not ‚Äú2‚Äù the value is 1 - sick.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre></td><td class="rouge-code"><pre><span class="n">Y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span> <span class="k">if</span> <span class="n">label</span> <span class="o">==</span> <span class="mi">2</span> <span class="k">else</span> <span class="mi">1</span> <span class="k">for</span> <span class="n">label</span> <span class="ow">in</span> <span class="n">dataset</span><span class="p">[</span><span class="s">"labels"</span><span class="p">]])</span>
<span class="n">Y</span><span class="p">.</span><span class="n">shape</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>Output:</p>

<div class="bg-gray-100 py-4 mb-6">
    <span>
    (1034,)
    </span>
</div>

<h2 id="implementing-the-nn">Implementing the NN</h2>

<p>We will define several helper functions that will implement the forward and the backward prop in the NN. These functions can account for different sizes of the network - different number of hidden layers and different number of nodes in each layer.</p>

<p><code class="language-plaintext highlighter-rouge">initialise_parameters(n_x, nn_shape) -&gt; parameters</code> - initialise parameters for all layers
<code class="language-plaintext highlighter-rouge">forward_prop(X, parameters)</code> - implements the forward propagation step
<code class="language-plaintext highlighter-rouge">compute_cost(Y, Yhat)</code> - computes cross-entropy loss
<code class="language-plaintext highlighter-rouge">back_prop</code> - implements the back propagation step</p>

<p>Other helper functions:
<code class="language-plaintext highlighter-rouge">relu</code> - RELU activation function
<code class="language-plaintext highlighter-rouge">sigmoid</code> - sigmoid activation function
<code class="language-plaintext highlighter-rouge">relu_deriv</code> - Computes \(\frac{\partial A}{\partial Z}\) where A is a RELU activation function
<code class="language-plaintext highlighter-rouge">sigmoid_deriv</code> - Computes \(\frac{\partial A}{\partial Z}\) where A is a \(\sigma\) activation function</p>

<p><code class="language-plaintext highlighter-rouge">X is (30000, 1034)</code> - model inputs. 1034 examples, each a vector of 30000
<code class="language-plaintext highlighter-rouge">Y is (1034, 1)</code> - outputs. Labels for each example. 0, 1 or 2.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
</pre></td><td class="rouge-code"><pre><span class="c1"># `nn_shape` describes the number of layers in the NN, the number of nodes in each layer and the activation function for each layer
# Note that the last layer has 1 output node since this is a binary classifier
</span><span class="n">nn_shape_binary</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">{</span><span class="s">"nodes"</span><span class="p">:</span> <span class="mi">4</span><span class="p">,</span> <span class="s">"activation"</span><span class="p">:</span> <span class="s">"relu"</span><span class="p">},</span>
    <span class="p">{</span><span class="s">"nodes"</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span> <span class="s">"activation"</span><span class="p">:</span> <span class="s">"relu"</span><span class="p">},</span>
    <span class="p">{</span><span class="s">"nodes"</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span> <span class="s">"activation"</span><span class="p">:</span> <span class="s">"relu"</span><span class="p">},</span>
    <span class="p">{</span><span class="s">"nodes"</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="s">"activation"</span><span class="p">:</span> <span class="s">"sigmoid"</span><span class="p">},</span>
<span class="p">]</span>

<span class="n">L</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">nn_shape_binary</span><span class="p">)</span>
<span class="n">m</span> <span class="o">=</span> <span class="n">Y</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

<span class="k">print</span><span class="p">(</span><span class="s">"Number of layers: "</span><span class="p">,</span> <span class="n">L</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Number of examples: "</span><span class="p">,</span> <span class="n">m</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>Output:</p>

<div class="bg-gray-100 py-4 mb-6">
    <span>
    Number of layers:  4
    <br />
    Number of examples:  1034
    </span>
</div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
</pre></td><td class="rouge-code"><pre><span class="k">def</span> <span class="nf">initialise_parameters</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">nn_shape</span><span class="p">):</span>
    <span class="n">parameters</span> <span class="o">=</span> <span class="p">{}</span>
    
    <span class="c1"># Number of features in an example. 30000 in this case
</span>    <span class="n">n_x</span> <span class="o">=</span> <span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">layer</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">nn_shape</span><span class="p">):</span>
        <span class="n">n_prev</span> <span class="o">=</span> <span class="n">n_x</span> <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">0</span> <span class="k">else</span> <span class="n">nn_shape</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">][</span><span class="s">"nodes"</span><span class="p">]</span>
        <span class="c1"># Use He initialisation for weights. Initially I used small random initialisation (random weights multiplied by 0.01) but it led to *significantly* worse performance.
</span>        <span class="n">parameters</span><span class="p">[</span><span class="sa">f</span><span class="s">"W</span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s">"</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="n">layer</span><span class="p">[</span><span class="s">"nodes"</span><span class="p">],</span> <span class="n">n_prev</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mf">2.</span> <span class="o">/</span> <span class="n">n_prev</span><span class="p">)</span>
        <span class="c1"># Initialise biases with zeros
</span>        <span class="n">parameters</span><span class="p">[</span><span class="sa">f</span><span class="s">"b</span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s">"</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">layer</span><span class="p">[</span><span class="s">"nodes"</span><span class="p">],</span> <span class="mi">1</span><span class="p">))</span>
    
    <span class="k">return</span> <span class="n">parameters</span>

<span class="n">params</span> <span class="o">=</span> <span class="n">initialise_parameters</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">nn_shape_binary</span><span class="p">)</span>

<span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">params</span><span class="p">.</span><span class="n">items</span><span class="p">():</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"</span><span class="si">{</span><span class="n">k</span><span class="si">}</span><span class="s">: </span><span class="si">{</span><span class="n">v</span><span class="p">.</span><span class="n">shape</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>Output:</p>

<div class="bg-gray-100 py-4 mb-6">
    <span>
    W1: (4, 30000)
    <br />
    b1: (4, 1)
    <br />
    W2: (3, 4)
    <br />
    b2: (3, 1)
    <br />
    W3: (2, 3)
    <br />
    b3: (2, 1)
    <br />
    W4: (1, 2)
    <br />
    b4: (1, 1)
    </span>
</div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
</pre></td><td class="rouge-code"><pre><span class="k">def</span> <span class="nf">relu</span><span class="p">(</span><span class="n">Z</span><span class="p">):</span>
    <span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">maximum</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">Z</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">A</span>

<span class="k">def</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="n">Z</span><span class="p">):</span>
    <span class="n">A</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">Z</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">A</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<h2 id="forward-propagation">Forward propagation</h2>

<p>In the forward propagation step, we compute two things for each layer:</p>

<ul>
  <li>Linear activation: \(Z^{[l]} = W^{[l]}A^{[l-1]} + b^{[l]}\)</li>
  <li>Activation function: \(A^{[l]} = g^{[l]}(Z^{[l]})\)</li>
</ul>

<p>where \(g^{[l]}\) is the activation function for layer \(l\).</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
</pre></td><td class="rouge-code"><pre><span class="n">activations</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s">"relu"</span><span class="p">:</span> <span class="n">relu</span><span class="p">,</span>
    <span class="s">"sigmoid"</span><span class="p">:</span> <span class="n">sigmoid</span>
<span class="p">}</span>

<span class="k">def</span> <span class="nf">forward_prop</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">nn_shape</span><span class="p">):</span>
    <span class="n">grads</span> <span class="o">=</span> <span class="p">{</span><span class="s">"A0"</span><span class="p">:</span> <span class="n">X</span><span class="p">}</span>
    
    <span class="k">for</span> <span class="n">l</span><span class="p">,</span> <span class="n">layer</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">nn_shape</span><span class="p">):</span>
        <span class="n">W</span> <span class="o">=</span> <span class="n">params</span><span class="p">[</span><span class="sa">f</span><span class="s">"W</span><span class="si">{</span><span class="n">l</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s">"</span><span class="p">]</span>
        <span class="n">b</span> <span class="o">=</span> <span class="n">params</span><span class="p">[</span><span class="sa">f</span><span class="s">"b</span><span class="si">{</span><span class="n">l</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s">"</span><span class="p">]</span>
        <span class="n">A_prev</span> <span class="o">=</span> <span class="n">grads</span><span class="p">[</span><span class="sa">f</span><span class="s">"A</span><span class="si">{</span><span class="n">l</span><span class="si">}</span><span class="s">"</span><span class="p">]</span>
        
        <span class="n">Z</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">A_prev</span><span class="p">)</span> <span class="o">+</span> <span class="n">b</span>
        <span class="n">A</span> <span class="o">=</span> <span class="n">activations</span><span class="p">[</span><span class="n">layer</span><span class="p">[</span><span class="s">"activation"</span><span class="p">]](</span><span class="n">Z</span><span class="p">)</span>
        
        <span class="n">grads</span><span class="p">[</span><span class="sa">f</span><span class="s">"Z</span><span class="si">{</span><span class="n">l</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s">"</span><span class="p">]</span> <span class="o">=</span> <span class="n">Z</span>
        <span class="n">grads</span><span class="p">[</span><span class="sa">f</span><span class="s">"A</span><span class="si">{</span><span class="n">l</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s">"</span><span class="p">]</span> <span class="o">=</span> <span class="n">A</span>
        
    <span class="k">return</span> <span class="n">grads</span>

<span class="n">grads</span> <span class="o">=</span> <span class="n">forward_prop</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">nn_shape_binary</span><span class="p">)</span>

<span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">grads</span><span class="p">.</span><span class="n">items</span><span class="p">():</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"</span><span class="si">{</span><span class="n">k</span><span class="si">}</span><span class="s">: </span><span class="si">{</span><span class="n">v</span><span class="p">.</span><span class="n">shape</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>Output:</p>

<div class="bg-gray-100 py-4 mb-6">
    <span>
    A0: (30000, 1034)
    <br />
    Z1: (4, 1034)
    <br />
    A1: (4, 1034)
    <br />
    Z2: (3, 1034)
    <br />
    A2: (3, 1034)
    <br />
    Z3: (2, 1034)
    <br />
    A3: (2, 1034)
    <br />
    Z4: (1, 1034)
    <br />
    A4: (1, 1034)
    </span>
</div>

<h2 id="cost-function">Cost function</h2>

<p>The cost function for a binary classifier is the cross-entropy loss:</p>

\[J = -\frac{1}{m} \sum_{i=1}^{m} y^{(i)} \log(a^{(i)}) + (1 - y^{(i)}) \log(1 - a^{(i)})\]

<p>where \(a^{(i)}\) is the predicted value for the \(i\)-th example and \(y^{(i)}\) is the true label for the \(i\)-th example.</p>

<p>The accuracy of the model can‚Äôt be derived directly from a cost function. We will compute it separately later.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
</pre></td><td class="rouge-code"><pre><span class="c1"># AL stands for A[L] - activation of the last layer
</span><span class="k">def</span> <span class="nf">compute_cost</span><span class="p">(</span><span class="n">Y</span><span class="p">,</span> <span class="n">AL</span><span class="p">,</span> <span class="n">m</span><span class="p">):</span>
    <span class="c1"># print("Compute cost shapes ", Y.shape, AL.shape)
</span>    <span class="c1"># Y = Y.reshape((m,))
</span>    <span class="c1"># AL = AL.reshape((m,))
</span>    <span class="n">result</span> <span class="o">=</span> <span class="o">-</span> <span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="n">m</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">Y</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="n">AL</span><span class="p">).</span><span class="n">T</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">((</span><span class="mi">1</span> <span class="o">-</span> <span class="n">Y</span><span class="p">),</span> <span class="n">np</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">AL</span><span class="p">).</span><span class="n">T</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>
    
<span class="n">compute_cost</span><span class="p">(</span><span class="n">Y</span><span class="p">,</span> <span class="n">grads</span><span class="p">[</span><span class="sa">f</span><span class="s">"A</span><span class="si">{</span><span class="n">L</span><span class="si">}</span><span class="s">"</span><span class="p">],</span> <span class="n">m</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>Output:</p>

<div class="bg-gray-100 py-4 mb-6">
    <span>
    0.6931471805599453
    </span>
</div>

<h2 id="back-propagation">Back propagation</h2>

<p>In the back propagation step the goal is to update the weights <code class="language-plaintext highlighter-rouge">W</code> and biases <code class="language-plaintext highlighter-rouge">b</code> in the network to minimise the cost function.</p>

<p>The weights and biases are updated using the gradients (partial derivatives) of the cost function with respect to the weights and biases:</p>

\[W^{[l]} = W^{[l]} - \alpha \frac{\partial J}{\partial W^{[l]}}\]

\[b^{[l]} = b^{[l]} - \alpha \frac{\partial J}{\partial b^{[l]}}\]

<p>where \(\alpha\) is the learning rate.</p>

<h3 id="last-layer-with-sigmoid-activation-function">Last layer with sigmoid activation function</h3>

<p>To compute the partial derivatives of the cost function with regards to <code class="language-plaintext highlighter-rouge">W</code> and <code class="language-plaintext highlighter-rouge">b</code>, we need to apply the chain rule.</p>

<p>The only variable that ‚Äúfeeds‚Äù into the cost function is <code class="language-plaintext highlighter-rouge">A[L]</code>. <code class="language-plaintext highlighter-rouge">A[L]</code> depends on <code class="language-plaintext highlighter-rouge">Z[L]</code>, <code class="language-plaintext highlighter-rouge">Z[L]</code> depends on <code class="language-plaintext highlighter-rouge">W[L]</code> and <code class="language-plaintext highlighter-rouge">b[L]</code>.</p>

<p><img src="/assets/posts/simple-nn-binary/simple-nn-diagram-1.png" alt="Diagram 1" /></p>

<p>This means that the partial derivative of <code class="language-plaintext highlighter-rouge">L</code> with respect to <code class="language-plaintext highlighter-rouge">W[L]</code> and <code class="language-plaintext highlighter-rouge">b[L]</code> is computed as follows:</p>

\[\frac{\partial J}{\partial W^{[L]}} = \frac{\partial J}{\partial A^{[L]}} \frac{\partial A^{[L]}}{\partial Z^{[L]}} \frac{\partial Z^{[L]}}{\partial W^{[L]}}\]

<p>Let‚Äôs break down each one of these components.</p>

<h4 id="dal">dA[L]</h4>

<p>The derivative of the cost function with respect to the activation of the last layer is computed as follows:</p>

\[\frac{\partial J}{\partial A^{[L]}} = -\frac{Y}{A^{[L]}} + \frac{1 - Y}{1 - A^{[L]}}\]

<h4 id="dal-wrt-zl">dA[L] w.r.t Z[L]</h4>

<p>The derivative of the activation function with respect to the linear activation of the last layer depends on the activation function <code class="language-plaintext highlighter-rouge">g[L]</code>. For the sigmoid activation function, it is computed as follows:</p>

\[\frac{\partial A^{[L]}}{\partial Z^{[L]}} = \sigma'(Z^[L]) = \sigma(1 - \sigma) = A^{[L]}(1 - A^{[L]})\]

<h4 id="dzl-wrt-wl">dZ[L] w.r.t W[L]</h4>

<p>Finally, the derivative of <code class="language-plaintext highlighter-rouge">Z[L]</code> with respect to <code class="language-plaintext highlighter-rouge">W[L]</code> is the activation of the previous layer <code class="language-plaintext highlighter-rouge">A[L-1]</code>:</p>

\[\frac{\partial Z^{[L]}}{\partial W^{[L]}} = A^{[L-1]}\]

<p>Putting it all together and simpifying:</p>

\[\frac{\partial J}{\partial W^{[L]}} = \frac{\partial J}{\partial A^{[L]}} \frac{\partial A^{[L]}}{\partial Z^{[L]}} \frac{\partial Z^{[L]}}{\partial W^{[L]}} = (-\frac{Y}{A^{[L]}} + \frac{1 - Y}{1 - A^{[L]}}) * A^{[L]}(1 - A^{[L]}) * A^{[L-1]} = (A^{[L]} - Y) * A^{[L-1]}\]

<h3 id="hidden-layers-with-relu-activation-function">Hidden layers with RELU activation function</h3>

<p>This is the formula I‚Äôve struggled with the most. The derivative <code class="language-plaintext highlighter-rouge">dZ[l]</code> where <code class="language-plaintext highlighter-rouge">l</code> is one of the hidden layers. Applying the chain rule, this is how we compute <code class="language-plaintext highlighter-rouge">dZ[l]</code>:</p>

<p><img src="/assets/posts/simple-nn-binary/simple-nn-diagram-2.png" alt="Diagram 2" /></p>

<p><code class="language-plaintext highlighter-rouge">dA[l]</code> will be multiplied item-wise with a tensor that is 1 where <code class="language-plaintext highlighter-rouge">Z[l]</code> is greater than 0 and 0 where <code class="language-plaintext highlighter-rouge">Z[l]</code> is less than or equal to 0. This is why we initialise <code class="language-plaintext highlighter-rouge">dZ[l]</code> to a copy of <code class="language-plaintext highlighter-rouge">dA[l]</code> and then set all values at the indexes where <code class="language-plaintext highlighter-rouge">Z[l]</code> is less than 0 to 0.</p>

<p>Now to compute <code class="language-plaintext highlighter-rouge">dA[l]</code>:</p>

<p><img src="/assets/posts/simple-nn-binary/simple-nn-diagram-3.png" alt="Diagram 3" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
</pre></td><td class="rouge-code"><pre><span class="k">def</span> <span class="nf">back_prop</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">grads</span><span class="p">,</span> <span class="n">nn_shape</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.001</span><span class="p">):</span>
    <span class="c1"># Number of examples
</span>    <span class="n">m</span> <span class="o">=</span> <span class="n">Y</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="c1"># Number of layers
</span>    <span class="n">L</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">nn_shape</span><span class="p">)</span>
    
    <span class="c1"># Iterate over the layers, last to first
</span>    <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">L</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">):</span>
        <span class="n">W</span> <span class="o">=</span> <span class="n">params</span><span class="p">[</span><span class="sa">f</span><span class="s">"W</span><span class="si">{</span><span class="n">l</span><span class="si">}</span><span class="s">"</span><span class="p">]</span>
        <span class="n">b</span> <span class="o">=</span> <span class="n">params</span><span class="p">[</span><span class="sa">f</span><span class="s">"b</span><span class="si">{</span><span class="n">l</span><span class="si">}</span><span class="s">"</span><span class="p">]</span>
        <span class="n">Z</span> <span class="o">=</span> <span class="n">grads</span><span class="p">[</span><span class="sa">f</span><span class="s">"Z</span><span class="si">{</span><span class="n">l</span><span class="si">}</span><span class="s">"</span><span class="p">]</span>
        <span class="n">A</span> <span class="o">=</span> <span class="n">grads</span><span class="p">[</span><span class="sa">f</span><span class="s">"A</span><span class="si">{</span><span class="n">l</span><span class="si">}</span><span class="s">"</span><span class="p">]</span>

        <span class="c1"># Compute dZ[l] for the last layer with the sigmoid activation function
</span>        <span class="k">if</span> <span class="n">nn_shape</span><span class="p">[</span><span class="n">l</span><span class="o">-</span><span class="mi">1</span><span class="p">][</span><span class="s">"activation"</span><span class="p">]</span> <span class="o">==</span> <span class="s">"sigmoid"</span><span class="p">:</span>
            <span class="n">dZ</span> <span class="o">=</span> <span class="n">A</span> <span class="o">-</span> <span class="n">Y</span>
        
        <span class="c1"># Compute dZ[l] for the hidden layers
</span>        <span class="k">if</span> <span class="n">nn_shape</span><span class="p">[</span><span class="n">l</span><span class="o">-</span><span class="mi">1</span><span class="p">][</span><span class="s">"activation"</span><span class="p">]</span> <span class="o">==</span> <span class="s">"relu"</span><span class="p">:</span>
            <span class="n">dZnext</span> <span class="o">=</span> <span class="n">grads</span><span class="p">[</span><span class="sa">f</span><span class="s">"dZ</span><span class="si">{</span><span class="n">l</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s">"</span><span class="p">]</span>
            <span class="n">Wnext</span> <span class="o">=</span> <span class="n">params</span><span class="p">[</span><span class="sa">f</span><span class="s">"W</span><span class="si">{</span><span class="n">l</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s">"</span><span class="p">]</span>
            <span class="n">dA</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">Wnext</span><span class="p">.</span><span class="n">T</span><span class="p">,</span> <span class="n">dZnext</span><span class="p">)</span>
            <span class="n">grads</span><span class="p">[</span><span class="sa">f</span><span class="s">"dA</span><span class="si">{</span><span class="n">l</span><span class="si">}</span><span class="s">"</span><span class="p">]</span> <span class="o">=</span> <span class="n">dA</span>
            
            <span class="n">dZ</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="n">dA</span><span class="p">,</span> <span class="n">copy</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
            <span class="n">dZ</span><span class="p">[</span><span class="n">Z</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
        
        <span class="n">Aprev</span> <span class="o">=</span> <span class="n">X</span> <span class="k">if</span> <span class="n">l</span> <span class="o">==</span> <span class="mi">1</span> <span class="k">else</span> <span class="n">grads</span><span class="p">[</span><span class="sa">f</span><span class="s">"A</span><span class="si">{</span><span class="n">l</span><span class="o">-</span><span class="mi">1</span><span class="si">}</span><span class="s">"</span><span class="p">]</span>
        <span class="n">dW</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="n">m</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">dZ</span><span class="p">,</span> <span class="n">Aprev</span><span class="p">.</span><span class="n">T</span><span class="p">)</span>
        <span class="n">db</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="n">m</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">dZ</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

        <span class="n">W</span> <span class="o">=</span> <span class="n">W</span> <span class="o">-</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">dW</span>
        <span class="n">b</span> <span class="o">=</span> <span class="n">b</span> <span class="o">-</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">db</span>
        
        <span class="n">params</span><span class="p">[</span><span class="sa">f</span><span class="s">"W</span><span class="si">{</span><span class="n">l</span><span class="si">}</span><span class="s">"</span><span class="p">]</span> <span class="o">=</span> <span class="n">W</span>
        <span class="n">params</span><span class="p">[</span><span class="sa">f</span><span class="s">"b</span><span class="si">{</span><span class="n">l</span><span class="si">}</span><span class="s">"</span><span class="p">]</span> <span class="o">=</span> <span class="n">b</span>
        <span class="n">grads</span><span class="p">[</span><span class="sa">f</span><span class="s">"dZ</span><span class="si">{</span><span class="n">l</span><span class="si">}</span><span class="s">"</span><span class="p">]</span> <span class="o">=</span> <span class="n">dZ</span>
    
    <span class="k">return</span> <span class="n">params</span>

<span class="n">back_prop</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">grads</span><span class="p">,</span> <span class="n">nn_shape_binary</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>I‚Äôm going to skip the output here, it‚Äôs a bunch of tensors.</p>

<h2 id="putting-it-all-together">Putting it all together</h2>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
</pre></td><td class="rouge-code"><pre><span class="n">nn_shape</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">{</span><span class="s">"nodes"</span><span class="p">:</span> <span class="mi">4</span><span class="p">,</span> <span class="s">"activation"</span><span class="p">:</span> <span class="s">"relu"</span><span class="p">},</span>
    <span class="p">{</span><span class="s">"nodes"</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span> <span class="s">"activation"</span><span class="p">:</span> <span class="s">"relu"</span><span class="p">},</span>
    <span class="p">{</span><span class="s">"nodes"</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span> <span class="s">"activation"</span><span class="p">:</span> <span class="s">"relu"</span><span class="p">},</span>
    <span class="p">{</span><span class="s">"nodes"</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="s">"activation"</span><span class="p">:</span> <span class="s">"sigmoid"</span><span class="p">},</span>
<span class="p">]</span>

<span class="n">L</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">nn_shape</span><span class="p">)</span>
<span class="n">m</span> <span class="o">=</span> <span class="n">Y</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

<span class="n">num_epochs</span> <span class="o">=</span> <span class="mi">1000</span>

<span class="n">params</span> <span class="o">=</span> <span class="n">initialise_parameters</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">nn_shape</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">num_epochs</span><span class="p">):</span>
    <span class="n">grads</span> <span class="o">=</span> <span class="n">forward_prop</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">nn_shape</span><span class="p">)</span>
    <span class="n">cost</span> <span class="o">=</span> <span class="n">compute_cost</span><span class="p">(</span><span class="n">Y</span><span class="p">,</span> <span class="n">grads</span><span class="p">[</span><span class="sa">f</span><span class="s">"A</span><span class="si">{</span><span class="n">L</span><span class="si">}</span><span class="s">"</span><span class="p">],</span> <span class="n">m</span><span class="p">)</span>
    <span class="n">params</span> <span class="o">=</span> <span class="n">back_prop</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">grads</span><span class="p">,</span> <span class="n">nn_shape</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">100</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Cost at #</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s">: </span><span class="si">{</span><span class="n">cost</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>Output:</p>

<div class="bg-gray-100 py-4 mb-6">
    <span>
    Cost at #0: 0.6931471805599453
    <br />
    Cost at #100: 0.5647623281046791
    <br />
    Cost at #200: 0.49435884342270875
    <br />
    Cost at #300: 0.46541043351056827
    <br />
    Cost at #400: 0.44768665629851173
    <br />
    Cost at #500: 0.4339341072340277
    <br />
    Cost at #600: 0.4226418290681421
    <br />
    Cost at #700: 0.4130181578936699
    <br />
    Cost at #800: 0.40420962864501486
    <br />
    Cost at #900: 0.3960244415732821
    </span>
</div>

<h2 id="measuring-accuracy">Measuring accuracy</h2>

<p>The accuracy of the model can not be directly derived from the cost function.</p>

<p>We will load the dev dataset and measure the accuracy of the model on the training and the dev set.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
</pre></td><td class="rouge-code"><pre><span class="n">dev</span> <span class="o">=</span> <span class="n">load_dataset</span><span class="p">(</span><span class="s">"beans"</span><span class="p">,</span> <span class="n">split</span><span class="o">=</span><span class="s">"validation"</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Dev dataset: "</span><span class="p">,</span> <span class="n">dev</span><span class="p">)</span>

<span class="n">dev</span> <span class="o">=</span> <span class="n">dev</span><span class="p">.</span><span class="nb">map</span><span class="p">(</span><span class="n">transform</span><span class="p">,</span> <span class="n">batched</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="n">X_dev</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="n">dev</span><span class="p">[</span><span class="s">"feature_vector"</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="s">'float32'</span><span class="p">).</span><span class="n">T</span>
<span class="k">print</span><span class="p">(</span><span class="s">"X_dev: "</span><span class="p">,</span> <span class="n">X_dev</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>

<span class="n">Y_dev</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span> <span class="k">if</span> <span class="n">label</span> <span class="o">==</span> <span class="mi">2</span> <span class="k">else</span> <span class="mi">1</span> <span class="k">for</span> <span class="n">label</span> <span class="ow">in</span> <span class="n">dev</span><span class="p">[</span><span class="s">"labels"</span><span class="p">]])</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>Output:</p>

<div class="bg-gray-100 py-4 mb-6">
    <span>
    Dev dataset:  Dataset({
    <br />
        features: ['image_file_path', 'image', 'labels'],
    <br />
        num_rows: 133
    <br />
    })
    <br />
    X_dev:  (30000, 133)
    </span>
</div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
</pre></td><td class="rouge-code"><pre><span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">nn_shape</span><span class="p">):</span>
    <span class="n">L</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">nn_shape</span><span class="p">)</span>

    <span class="n">grads</span> <span class="o">=</span> <span class="n">forward_prop</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">nn_shape</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">grads</span><span class="p">[</span><span class="sa">f</span><span class="s">"A</span><span class="si">{</span><span class="n">L</span><span class="si">}</span><span class="s">"</span><span class="p">]</span>

<span class="k">def</span> <span class="nf">accuracy</span><span class="p">(</span><span class="n">Y</span><span class="p">,</span> <span class="n">Y_pred</span><span class="p">):</span>
    <span class="n">Y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">Y</span><span class="p">,</span> <span class="p">(</span><span class="n">Y</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],))</span>
    <span class="n">Y_pred</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">Y_pred</span><span class="p">,</span> <span class="p">(</span><span class="n">Y</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],))</span>
    <span class="n">Y_pred_bin</span> <span class="o">=</span> <span class="p">(</span><span class="n">Y_pred</span> <span class="o">&gt;=</span> <span class="mf">0.5</span><span class="p">).</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">Y</span> <span class="o">==</span> <span class="n">Y_pred_bin</span><span class="p">)</span> <span class="o">/</span> <span class="n">Y</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
</pre></td><td class="rouge-code"><pre><span class="n">Y_pred</span> <span class="o">=</span> <span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">nn_shape</span><span class="p">)</span>
<span class="n">Y_dev_pred</span> <span class="o">=</span> <span class="n">predict</span><span class="p">(</span><span class="n">X_dev</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">nn_shape</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="s">"Accuracy on the training set: "</span><span class="p">,</span> <span class="n">accuracy</span><span class="p">(</span><span class="n">Y</span><span class="p">,</span> <span class="n">Y_pred</span><span class="p">.</span><span class="n">T</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Accuracy on the dev set: "</span><span class="p">,</span> <span class="n">accuracy</span><span class="p">(</span><span class="n">Y_dev</span><span class="p">,</span> <span class="n">Y_dev_pred</span><span class="p">.</span><span class="n">T</span><span class="p">))</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>Output:</p>

<div class="bg-gray-100 py-4 mb-6">
    <span>
    Accuracy on the training set:  0.8520309477756286
    <br />
    Accuracy on the dev set:  0.8646616541353384
    </span>
</div>

<p>That‚Äôs it! Not a bad result - ~85% accuracy on both the training and the dev set - for a small network working on a compressed images and without any optimisation.</p>

<p>In the next post we‚Äôll change the classifier to a multi-class one using the softmax function.</p>

</div>


    </div>
  </div>
</body>
